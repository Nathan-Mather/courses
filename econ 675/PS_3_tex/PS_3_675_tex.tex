%-----------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------



\documentclass[11pt]{article}

\usepackage[top=2cm, bottom=3cm, left=2cm, right=2cm]{geometry}

\setlength{\parindent}{0in}

\newcommand{\Var}{\mathrm{Var}}

\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\plim}{\rightarrow_{p}}

\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{bm}
\usepackage{listings}

% Expectation symbol
\newcommand{\E}{\mathrm{E}}
\newcommand{\V}{\mathrm{V}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}} 

%----------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------

\title{Econ 675 Assignment 1} % The article title


\author{Nathan Mather\thanks{Shouts out to Ani, Paul, Tyler, Erin, Caitlin and others for all the help with this } } % The article author(s) 

\date{\today} % An optional date to appear under the author(s)


%----------------------------------------------------------------------------------
\begin{document}
	
%------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%------------------------------------------------------------------------------
\maketitle % Print the title/author/date block

\setcounter{tocdepth}{2} % Set the depth of the table of contents to show sections and subsections only

\tableofcontents % Print the table of contents


%-------------------------------------------------------------
% Question 1 
%-------------------------------------------------------------
\section{Question 1: Non-linear Least Squares}
\subsection{Q1 Part 1}

The general non-linear least squares estimator is 
$$ \bm{\hat{\beta}}_n = \arg \min_{\bm{\beta} \in \R^d} \frac{1}{n} \sum_{i=1}^{n}(y_i - \mu(\bm{x_i' \beta}))^2
$$

Now for $  \bm{\beta}_0 = \arg \min_{\bm{\beta} \in \R^d} \E[ (y_i - \mu(\bm{x_i' \beta}))^2]$ to be identifiable we need:

$$ \bm{\beta}_0 =  \bm{\beta}_0^* $$



$$ \iff  \bm{\beta}^*_0 = \arg \min_{\bm{\beta} \in \R^d} \E[ (y_i - \mu(\bm{x_i' \beta}))^2]$$ 

To find this start by noting that 

$$ \E[(y_i - \mu(\bm{x}'_i\bm{\beta}))^2] = \E[(y_i-\mu(\bm{x}'_i\bm{\beta}_0) +\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta})  )^2]
$$

$$ = \E[(y_i-\mu(\bm{x}'_i\bm{\beta}_0))^2] + \E[(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta}) )^2] + 2\E[(y_i-\mu(\bm{x}'_i\bm{\beta}_0))(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta})) ]
$$

$$
=\E[(y_i-\mu(\bm{x}'_i\bm{\beta}_0))^2] + \E[(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta}) )^2]
$$

The last equality comes from the last term being zero by iterated expectations. I show this below. 

$$
\E[(y_i-\mu(\bm{x}'_i\bm{\beta}_0))(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta}))] = \E[\E[(y_i-\mu(\bm{x}'_i\bm{\beta}_0))(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta})) ]| \bm{x}_i]
$$

$$
= \E[(\E[y_i| \bm{x}_i]-\mu(\bm{x}'_i\bm{\beta}_0))(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta}))] = 0
$$

Using this fact we have that 

$$
\E[(y_i - \mu(\bm{x}'_i\bm{\beta}))^2] =\E[(y_i-\mu(\bm{x}'_i\bm{\beta}_0))^2] + \E[(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta}) )^2] \geq\E[(y_i-\mu(\bm{x}'_i\bm{\beta}_0))^2]
$$

$$ \forall \bm{\beta} \neq \bm{\beta}_0
$$

This is strictly greater than unless $\exists \bm{\beta} \neq \bm{\beta}_0 $ such that $\E[(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta}) )^2] = 0 $ Thus this give us an identification condition that $\E[(\mu(\bm{x}'_i\bm{\beta}_0) -\mu(\bm{x}'_i\bm{\beta}) )^2] \neq 0$ $\forall \bm{\beta} \neq \bm{\beta}_0 $. This means that $\bm{\beta}_0$ is the unique minimizer of $\E[ (y_i - \mu(\bm{x_i' \beta}))^2]$

Next note that if $\mu(\cdot)$ is a linear function, $\bm{\beta}_0$ is the coefficient of the best linear predictor and has the usual closed form $\bm{\beta}_0 = \E[\bm{x}_i \bm{x}'_i]^{-1} \E[\bm{x}_i y_i]
$

\subsection{Q1 Part 2}

In order to set this up as a Z estimator lets take a first order condition. This gives use the following condition. 

$$ \E[(\mu(\bm{x}'_i\bm{\beta}_0) - \mu(\bm{x}'_i \bm{\beta})) \dot{\mu}(\bm{x}'_i\bm{\beta})\bm{x}_i] = 0 
$$

Now take the sample analog and let $m(\bm{z}_i, \bm{\beta}) = (y_i - \mu(\bm{x}'_i \bm{\beta})) \dot{\mu}(\bm{x}'_i\bm{\beta})\bm{x}_i $ where $\bm{z}_i = (y_i, \bm{x}'_i)'$. We can write $\hat{\bm{\beta}}_n$ as the Z-estimator that solves:

$$ 0 = \frac{1}{n} \sum_{i=1}^{n} m(\bm{z}_i, \hat{\bm{\beta}}_n)
$$

Now assuming $ \hat{\bm{\beta}}_n \to \bm{\beta}_0 $ and regularity conditions we get the standard M estimation result.

$$ \sqrt{n}(\hat{\bm{\beta}}_n - \bm{\beta}_0) \to_d \N(0, \bm{H_0^{-1} \Sigma_0 H_0^{-1}})
$$ 
Were 
$$
\bm{H}_0 = \E \left[ \frac{\partial}{\partial \bm{\beta}} m(\bm{z}_i, \bm{\beta}_0) \right] = \E[ \dot{\mu}(\bm{x}'_i \bm{\beta}_0)^2\bm{x_ix'_i} ]
$$

and 
$$
\bm{\Sigma_0} = \V[m(\bm{z}_i, \bm{\beta}_0)] = \E[\sigma^2(\bm{x_i}) \dot{\mu}(\bm{x'_i \beta_o})^2 \bm{x_ix'_i}]
$$


\subsection{Q1 Part 3}

$$\hat{\V}_n^{HC} = \left( \frac{1}{n} \sum_{i=1}^{n} \hat{\bm{m}} \hat{\bm{m}}'  \right)^{-1}  \left( \frac{1}{n} \sum_{i=1}^{n} \hat{\bm{m}} \hat{\bm{m}}' \hat{e}_i^2  \right) \left( \frac{1}{n} \sum_{i=1}^{n} \hat{\bm{m}} \hat{\bm{m}}'  \right)^{-1} 
$$

where $\hat{\bm{m}} = \bm{m_{\beta}(z_i, \hat{\beta})} $  and $ hat{e} = y_i -\bm{m(z_i, \hat{\beta})} $

The confidence interval is 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

% do the conf interval  do the conf interval do the conf interval do the conf interval do the conf interval do the conf interval do the conf interval do the conf interval do the conf interval 

\subsection{Q1 Part 4}

In this case we get $\bm{\Sigma_0} = \sigma^2 \bm{H}_0$ and the asymptotic variance reduces to 

$$ \bm{V}_0 = \sigma^2 \bm{H}_0 ^{-1} = \sigma^2 \E[ \dot{\mu}(\bm{x}'_i \bm{\beta}_0)^2\bm{x_ix'_i} ]^{-1} $$

We can estimate variane using $\hat{\bm{V}} = \hat{\sigma}^2 \hat{\bm{H}}^{-1}$ where 

$$ \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n}(y_i - \mu(\bm{x}'_i \hat{\bm{\beta}}_n))^2
$$
and 
$$ \hat{\bm{H}} = \frac{a}{n} \sum_{i=1}^{n}\dot{\mu}(\bm{x}'_i \bm{\beta}_0)^2\bm{x_ix'_i}
$$

Which is consistent by the continuous mapping theorem. Now by the delta method and letting $g(\bm{\beta}) = \lVert \bm{\beta} \rVert = \sum_{k=1}^{d} \bm{\beta}_k^2
$ we get 

$$ \sqrt{n}(g(\hat{\bm{\beta_n}}) - g(\bm{\beta}_0)) \to_d \N (0, \dot{g}(\bm{\beta}_0) \bm{V}_0 \dot{g}(\bm{\beta}_0)')
$$

where $\dot{g}(\bm{\beta}_0) = \frac{d}{d\bm{\beta}'}g(\bm{\beta}) = 2 \bm{\beta}' $ Hence the confidence interval is given by 


$$ CI_{0.95} = \left[ \lVert \hat{\bm{\beta}}_n \rVert ^2 - 1.96 \sqrt{\frac{4\hat{\bm{\beta_n}}'\hat{\bm{V}} \hat{\bm{\beta_n}} }{n}},  \lVert \hat{\bm{\beta}}_n \rVert ^2 + 1.96 \sqrt{\frac{4\hat{\bm{\beta_n}}'\hat{\bm{V}} \hat{\bm{\beta_n}} }{n}} \right] 
$$

\subsection{Q1 Part 5}
The conditional likelihod funciton is 

$$f_{y|x}(y_i|\bm{x}_i) = \frac{1}{(2\pi)^{n/2} \sigma^2} exp \left( -\frac{1}{2 \sigma^2} \sum_{i=1}^{n}(y_i - \mu(\bm{x}_i'\bm{\beta}))^2 \right)
$$

with log likelihood 

$$ \ell_n(\bm{\beta}, \sigma^2) = -\frac{1}{2\sigma^2} \sum_{i=1}^{n}(y_i - \mu(\bm{x}_i' \bm{\beta}))^2 - \frac{n}{2}log(\sigma^2)
$$

This gives us the following first order conditions 

$$ \frac{\partial}{\partial \bm{\beta}} \ell_n(\bm{\beta}, \sigma^2) = \frac{1}{\hat{\sigma}_{ML}^2} \sum_{i=1}^{n}(y_i - \mu(\bm{x}_i' \bm{\beta}_{ML}))\dot{\mu}(\bm{x}_i' \bm{\beta}_{ML})\bm{x}_i = 0
$$

$$\frac{\partial}{\partial \sigma^2} \ell_n(\bm{\beta}, \sigma^2) =
\frac{1}{2\hat{\sigma}_{ML}^4} \sum_{i=1}^{n}(y_i - \mu(\bm{x}_i' \bm{\beta}_{ML}))^2 - \frac{n}{2 \hat{\sigma}_{ML}^2} =0
$$

These conditions are equivalent to those found above. 

\subsection{Q1 Part 6}

%------------------------------------------------
% end doc
%------------------------------------------------
\end{document}

